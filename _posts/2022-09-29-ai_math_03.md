---
title: '[AI Math] 경사하강법 - 순한맛'
excerpt_separator: <!--more-->
categories:
  - 'Boostcamp AI Tech'
tags:
  - 'gradient descent'
use_math: true
---

## 경사하강법

### 미분 (differentiation)
- 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구  
    
  $$f'(x) = \lim_{h \to 0}\frac{f(x+h)-f(x)}{h}$$
    
- 미분은 함수 $f$의 주어진 점 $(x, f(x))$에서의 접선의 기울기를 구한다.
- 접선의 기울기를 알게 되면 점을 어느 방향으로 움직여야 함수값이 증가하는지, 또는 감소하는지 알 수 있다. 
  - 증가시키고 싶다면 미분값을 더하기 (경사상승법-gradient ascent)
    - 목적함수를 최대화할 때 사용
  - 감소시키고 싶다면 미분값을 빼주기 (경사하강법-gradient descent)
    - 목적함수를 최소화할 때 사용
  - 경사상승/하강법은 극값에서는 미분값이 0이 되므로 도달하게 되면 움직임을 멈추게 된다.
    
### 경사하강법: 알고리즘
```py
# gradient: 미분을 계산하는 함수
# init: 시작점, lr: 학습률, eps: 알고리즘 종료조건

var = init
grad = gradient(var)
while norm(grad) > eps: # 컴퓨터 계산에서 정확히 0이 되는 것은 불가능하므로 종료 조건(eps)을 설정
  var = var - lr * grad # lr은 경사하강법의 속도를 조절해주는 학습률
  grad = gradient(var) # 종료조건이 성립할 때까지 미분값 업데이트
```

### 변수가 벡터인 경우 
- 편미분(partial differentiation)을 사용  

  $$\partial_{x_i} f(x) = \lim_{h \to 0}\frac{f(x+he_i)-f(x)}{h}$$ 
  
  $e_i$는 i번째 값만 1이고 나머지는 0인 단위벡터
- 각 변수 별로 편미분을 계산한 그레디언트(gradient) 벡터를 이용하여 경사하강/경사상승법에 사용할 수 있다.

  $$\nabla f = (\partial_{x_1}f, ... \partial_{x_d}f)$$  
    
### 그레디언트 벡터 (gradient vector)

$$ f(x, y) = x^2 + 2y^2 $$  

- 경사상승법의 그레디언트 벡터

$$\nabla f = (2x, 4y)$$    

- 경사하강법의 그레디언트 벡터

$$-\nabla f = -(2x, 4y)$$  
