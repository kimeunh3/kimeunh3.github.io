---
title: '[AI Math] 통계학 맛보기'
excerpt_separator: <!--more-->
categories:
  - 'Boostcamp AI Tech'
tags:
  - 'parameter'
  - 'maximum likelihood estimation'
  - 'kullback-leibler divergence'
use_math: true
---

### 통계적 모델링
- 적절한 가정 위에서 확률분포를 추정(inference)하는 것이 목표
- 기계학습과 통계학이 공통적으로 추구하는 목표
- 유한한 개수의 데이터만으로 모집단의 분포를 알아내는 것은 불가능, 근사적으로 확률분포를 추정할 수 밖에 없다.
    - 예측모형의 목적은 분포를 정확하게 맞추는 것이 아닌 데이터와 추정 방법의 불확실성을 고려하여 위험을 최소화하는 것이다.

### 모수
- 데이터가 특정 확률 분포를 따른다고 가정한 후, 그 분포를 결정하는 모수(parameter)를 추정하는 방법을 모수적(parametric) 방법론이라 한다.
- 특정 확률분포를 가정하지 않고, 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면 비모수(nonparametric) 방법론이라 한다.

### 확률분포 가정하기
- 히스토그램을 통해 모양을 관찰한다.
- 데이터를 생성하는 원리를 먼저 고려하는 것이 원칙
    - 각 분포마다 검정방법이 있으므로 모수를 추정한 후에는 반드시 검정 필요
    
### 데이터로 모수 추정
- 확률분포를 가정했다면 모수 추정 가능
    - 정규분포의 모수는 평균 $\mu$ 과 분산 $\sigma^2$이다.
        - 통계량 = 모수인 평균과 분산을 추정하는 표본평균과 표본분산
- 모수를 추정하는 통계량의 확률분포를 표집분포(sampling distribution)이라고 한다.
    - 표본평균의 표집분포는 N이 커질수록 정규분포 $\mathcal N(\mu, \sigma^2/N)$를 따른다. $\rightarrow$ 중심극한정리(Central Limit Theorem)
        - 모집단의 분포와 상관없이 성립
    - 표본분포(sample distribution)과는 다른 개념! 표본분포는 아무리 데이터를 많이 모아도 정규분포가 될 수 없다!
    
### 최대가능도 추정법 (maximum likelihood estimation, MLE)
- 확률분포마다 사용하는 모수가 다르므로 적절한 통계량이 달라지게 되기 때문에 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나  
    
    $$\hat{\theta}_{\operatorname{MLE}} = \operatorname{argmax}L(\theta; x) = \operatorname{argmax}P(x|\theta)$$  
    
    - 가능도(likelihood) 함수는 모수 $\theta$를 따르는 분포가 $x$를 관찰할 가능성을 뜻하지만 확률로 해석하면 X
- 데이터 집합 $X$가 독립적으로 추출되었을 경우 로그가능도를 최적화한다.
    
    $$L(\theta; X) = \prod_{i=1}^n P(x_i|\theta) \Rightarrow \log L(\theta; X) = \prod_{i=1}^n \log P(x_i|\theta)$$

### 왜 로그가능도?
- 데이터의 숫자가 수억 단위가 된다면 컴퓨터의 정확도로는 가능도 계산 불가능
- 데이터가 독립일 경우, 로그를 사용하여 곱셈이었던 연산을 덧셈으로 바꿔주어 컴퓨터로 연산이 가능하게 만들어주는 것
- 경사하강법으로 가능도로 최적화할 때 미분 연산을 사용하는데, 로그가능도를 사용하면 연산량을 $O(n^2)$에서 $O(n)$으로 줄여준다.
    - 경사하강법에서는 음의 로그가능도(negative log-likelihood)를 최적화
    
### 딥러닝에서 최대가능도 추정법

$$\theta = (W^{(1)}, ... , W^{(L)})$$

$$y = (y_1, ... , y_K)$$

$$\hat{\theta}_{\operatorname{MLE}} = \operatorname{argmax} \frac{1}{n} \sum_{i=1}^n \sum_{k=1}^K y_{i,k} \log (\operatorname{MLP}_{\theta}(x_i)_k)$$

### 확률분포의 거리 구하기
- 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도
- 두 확률분포 사이의 거리(distance)를 계산하는 함수
    - 총 변동 거리 (Total Variation Distance, TV)
    - 쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL)
    - 바슈타인 거리 (Wasserstein Distance)

### 쿨백-라이블러 발산
- 이산확률변수  
    
    $$\mathbb{KL}(P \parallel Q) = \sum_{x \in \mathcal X} P(x) \log \left( \frac{P(x)}{Q(x)} \right)$$

- 연속확률변수  
    
    $$\mathbb{KL}(P \parallel Q) = \int_{\mathcal X} P(x) \log \left( \frac{P(x)}{Q(x)} \right ) dx$$

- 다음과 같이 분해 가능  
    
    $$\mathbb{KL}(P \parallel Q) = -\mathbb E_{x~P(x)} [\log {Q(x)}] + \mathbb E_{x~P(x)} [\log {P(x)}]$$
    
    - $-\mathbb E_{x~P(x)} [\log {Q(x)}]$ = 교차엔트로피(cross entropy)
    - $-\mathbb E_{x~P(x)} [\log {P(x)}]$ = 엔트로피(entropy)
    - 결과적으로 교차엔트로피에서 엔트로피를 뺀 값이 된다.
- 분류 문제에서 정답레이블을 P, 모델 예측을 Q라고 두게 되면 MLE는 교차엔트로피의 -를 씌운 값을 최대화 하는 값과 같기 때문에 결과적으로 쿨백-라이블러 발산을 최소화하는 것과 같게 된다.

    