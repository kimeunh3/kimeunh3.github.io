---
title: '[AI Math] 확률론 맛보기'
excerpt_separator: <!--more-->
categories:
  - 'Boostcamp AI Tech'
tags:
  - 'probability'
  - 'probability distribution'
  - 'discrete variable'
  - 'continuous variable'
  - 'conditional probability'
  - 'expectation'
  - 'monte carlo sampling'
use_math: true
---

### 딥러닝과 확률론
- 딥러닝은 확률론 기반의 기계학습 이론에 바탕
- 손실함수(loss function)들의 작동 원리는 데이터 공간을 통계적으로 해석해서 유도
  - 예측이 틀릴 위험을 최소화하도록 데이터를 학습
- 회귀분석의 손실함수를 계산할 때 쓰이는 L2-norm은 예측오차의 분산을 가장 최소화하는 방향으로 학습 유도
- 분류문제의 교차엔트로피(cross-entropy)는 모델 예측의 불확실성을 최소화하는 방향으로 학습 유도
- 분산 또는 불확실정을 최소화하기 위해서는 측정하는 방법을 알아야 하고, 그 측정법을 통계학에서 제공하기 때문에 기계학습을 이해하려면 확률론의 기본 개념을 알아야한다.

### 확률분포 = 데이터의 초상화
- 데이터 공간 = $\mathcal X \times \mathcal Y = \mathcal D$
- 데이터 $(x, y) \sim \mathcal D$
  - $(x, y) \in \mathcal X \times \mathcal Y$ (관측 가능한 데이터)
- 결합분포 (joint distribution)
  - 확률분포가 어떤 확률 분포를 가지고 있었냐에 상관없이 모델링에 따라 사용하는 결합분포가 달라질 수 있다.
  - 주어진 데이터의 모양을 보고 적절하게 이산, 또는 연속형의 결합분포 선택 가능
- 주변확률분포
  - $P(x)$ = x에 대한 주변확률분포
    - 결합분포 $P(x, y)$를 y에 대해서 더해주거나 적분하여 구한다.
- 조건부확률분포
  - $P(x \mid y)$ = 특정 클래스가 주어진 조건에서 데이터의 확률분포
    
### 이산확률변수 vs. 연속확률변수
- 확률변수는 확률분포 $\mathcal D$에 따라 이산형(discrete)과 연속형(continuous)로 구분
- 이산형 확률변수
  - 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해 모델링  
  
  $$\mathbb P (X \in A) = \sum_{x \in A} P(X = x)$$
  
    - $P(X = x)$ = 확률변수가 x값을 가질 확률
- 연속형 확률변수
  - 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링  
  
  $$\mathbb P (X \in A) = \int_A P(x)dx$$
  
    - $P(x)$ = 누적확률분포의 변화율 = $\lim_{h \to 0} \frac{\mathbb P (x-h \leq X \leq x+h)}{2h}$  

### 조건부확률
- $P(y \mid x)$ = 입력변수 x에 대해 정답이 y일 확률
  - 연속확률분포의 경우 확률이 아니라 밀도로 해석
- 선형모델과 소프트맥스 함수의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데 사용
  - 분류문제에서의 $softmax(W\phi + b$)는 데이터 x로부터 추출된 특징패턴 $\phi(x)$과 가중치행렬 $W$을 통해 조건부확률 $P(y \mid x)$을 계산
- 회귀문제의 경우 조건부기대값 $\mathbb E [y \mid x]$을 추정
  - 보통 연속확률변수를 다루기 때문에 밀도함수로 해석
  - 회귀문제의 손실함수는 L2-norm의 기대값인데 조건부기대값은 그 L2-norm을 최소화하는 함수와 일치하기 때문에 사용한다. 
- 딥러닝은 다층신경망을 이용하여 데이터로부터 특징패턴 $\phi$을 추출
    
### 기대값

$$\mathbb E_{x~P(x)}[f(x)] = \int_{\mathcal X} f(x)P(x)dx,\quad \mathbb E_{x~P(x)}[f(x)] = \sum_{x \in \mathcal X} f(x)P(x)$$  

- 확률분포가 주어지면 데이터를 분석하는데 사용 가능한 통계적 범함수(statistical functional)를 계산 가능
- 기대값(expectation) = 데이터를 대표하는 통계량
  - 통계적 범함수를 계산하는데 사용
- 분산, 첨도, 공분산 등 여러 통계량을 계산 가능

### 몬테카를로 샘플링
- 확률분포를 모를 때 데이터를 이용하여 기대값을 계산하기 위해 쓰이는 방법  
    
  $$\mathbb E_{x~P(x)}[f(x)] \approx \frac{1}{N} \sum_{i = 1}^N f(x^{(i)})$$
  
  - 이산형이든 연속형이든 상관없이 성립
- 독립추출(데이터끼리 상관관계 없이 데이터를 추출)만 보장된다면 대수의 법칙(law of large number)에 의해 수렴성 보장
- 샘플사이즈가 작을 경우, 오차범위가 커질 수 있으므로 적절한 샘플사이즈를 찾아주어야 한다.
    