---
title: '[AI Math] 딥러닝 학습방법 이해하기'
excerpt_separator: <!--more-->
categories:
  - 'Boostcamp AI Tech'
tags:
  - 'neural network'
  - 'softmax'
  - 'activation function'
  - 'backpropagation'
use_math: true
---

### 비선형 모델 신경망(neural network) 
- 선형모델로는 분류문제나 복잡한 패턴의 문제를 풀기 힘들다.
- 그런 모델을 풀기 위해 비선형모델인 신경망(neural network)를 사용
- 신경망은 선형모델과 활성함수(activation function)를 합성한 함수
    - 선형모델로 나온 출력물을 비선형모델로 만들어주는 것이 활성함수
    - 비선형모델로 변형되어 나온 벡터를 잠재(hidden)벡터라고 한다.
    - 잠재벡터 하나를 뉴런(neuron)이라고 하고 이 여러가지 뉴런이 모여 이루어진 모델이 신경망
- $W_1$와 $b_1$를 거쳐 나온 잠재벡터 H에서 한번 더 $W_2$와 $b_2$를 통해 선형변환하여 출력하게 된다면 $(W_1, W_2)$를 parameter로 가진 2-layers 신경망이 된다.
    - 이 작업을 반복적으로 진행하기 되면 다층(multi-layer) 퍼셉트론(MLP)가 된다. (딥러닝의 가장 기본적인 모형)
    - 1부터 L까지의 순차적인 신경망 계산을 순전파(forward propagation)이라 부른다.
        - 학습이 아닌 주어진 입력이 왔을 때 출력물을 내뱉는 과정의 연산

### 소프트맥스 연산  

$$softmax(o) = \left( \frac{exp(o_1)}{\sum_{k=1}^p exp(o_k)},\ \cdots\ , \frac{exp(o_p)}{\sum_{k=1}^p exp(o_k)} \right)$$

- 특정 클래스 k에 속할 확률 
- 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산
- 분류문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측

$$softmax(o) = softmax(Wx+b)$$

- 추론할 때는 출력값에서 가장 최대값을 가지는 주소만 1로 출력하는 원-핫(one-hot) 벡터를 사용한다. 소프트맥스는 학습할 때 쓰임!

### 활성함수
- 실수 값을 입력으로 받아 실수 값을 출력하는 비선형(nonlinear) 함수
- 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 X
- 시그모이드(sigmoid) 함수, tanh 함수가 전통적으로 많이 쓰이지만 딥러닝에선 ReLU가 많이 쓰임
![image](https://user-images.githubusercontent.com/59808674/194135314-3654792c-3634-4ba4-b0f0-8ea34eb47b8a.png)  

### 왜 층을 여러개를 쌓나요?
- 이론적으로는 2층 신경망으로도 가능 (universal approximation theorem)
- 그러나 층이 깊을수록 필요한 뉴런의 숫자가 훨씬 빨리 줄어들어 좀더 효율적으로 학습 가능
    - 층이 얇으면 뉴런의 숫자가 늘어나 넓은(wide) 신경망이 되어야 한다.
- 층이 깊다고 해서 최적화가 쉬운 것은 아니다. 층이 깊어지면 깊어질수록 학습하기 어려워질 가능성 $\uparrow$  

### 역전파 알고리즘
- L부터 1까지 역순으로 각 층의 그레디언트 벡터를 계산
- 연쇄법칙(chain-rule)기반 자동미분(auto-differentiation)을 사용